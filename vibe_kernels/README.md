# Vibe Kernels

High-performance GPU kernels for deep learning, generated by AI agents using Triton and CuTeDSL. 


## Quick Start

```python
import torch

from vibe_kernels import (
    fused_attention,
    triton_gemm,
    RMSNorm,
    apply_rotary_embedding,
    cross_entropy_loss,
    TritonAdamW,
)

# CUDA tensors (H100/SM90 recommended for best performance)
device = torch.device("cuda")
dtype = torch.float16

# Attention inputs: (batch, heads, seq_len, head_dim) in bf16/fp16
B, H, T, D = 1, 2, 64, 64
q = torch.randn(B, H, T, D, device=device, dtype=dtype)
k = torch.randn(B, H, T, D, device=device, dtype=dtype)
v = torch.randn(B, H, T, D, device=device, dtype=dtype)

# Fused attention with GQA support
out = fused_attention(q, k, v, causal=True)

# Rotary embeddings (cos/sin: (max_position, head_dim//2))
pos = torch.arange(T, device=device, dtype=dtype).unsqueeze(1)
cos = torch.cos(pos * torch.ones(D // 2, device=device, dtype=dtype))
sin = torch.sin(pos * torch.ones(D // 2, device=device, dtype=dtype))
q, k = apply_rotary_embedding(q, k, cos, sin, backend="torch")

# High-performance GEMM
a = torch.randn(32, 64, device=device, dtype=dtype)
b = torch.randn(64, 48, device=device, dtype=dtype)
bias = torch.randn(48, device=device, dtype=dtype)
out = triton_gemm(a, b, bias=bias)

# RMSNorm module
rms = RMSNorm(D, dtype=dtype, device=device)
y = rms(torch.randn(4, D, device=device, dtype=dtype))

# Fused cross-entropy loss
logits = torch.randn(16, 128, device=device, dtype=dtype)
targets = torch.randint(0, logits.shape[-1], (logits.shape[0],), device=device, dtype=torch.int64)
loss = cross_entropy_loss(logits, targets)

# Fused optimizer
p = torch.nn.Parameter(torch.randn(8, device=device, dtype=torch.float32))
opt = TritonAdamW([p], lr=1e-3)
p.sum().backward()
opt.step()
```

## Kernels

| Module | Key Exports | Description |
|--------|-------------|-------------|
| `attention` | `fused_attention` | Flash attention with causal/GQA support, Hopper optimizations |
| `gemm` | `triton_gemm`, `GEMMTiling` | Hopper TMA path, warp specialization, FP16/BF16 |
| `rmsnorm` | `RMSNorm` | RMS normalization with fused backward |
| `layernorm` | `layernorm`, `CuTeDSLLayerNorm` | Layer normalization (Triton + CuTeDSL backends) |
| `rotary` | `apply_rotary_embedding` | Rotary position embeddings |
| `embedding` | `FusedEmbeddingRMSNorm` | Fused embedding lookup + RMSNorm |
| `loss` | `cross_entropy_loss` | Fused cross-entropy with softmax |
| `softmax` | `softmax`, `log_softmax` | Multi-backend softmax (Triton/CuTeDSL) |
| `activation` | `relu_squared`, `softcap_tanh_projection`, `elementwise_*` | Activation functions |
| `optim` | `TritonAdamW`, `TritonMuon`, `clip_grad_norm_` | Fused optimizers |
| `indexing` | `gather`, `scatter_add`, `scatter_add_` | Indexing operations |
| `sampling` | `sample_logits` | Top-k/top-p sampling |
| `vibe_attention` | Flash attention implementation | SM90/SM100 optimized attention |

## Directory Structure

```
vibe_kernels/
├── activation/      # Elementwise ops (ReLU², softcap, etc.)
├── attention/       # Fused causal attention with GQA
├── embedding/       # Token embedding + RMSNorm fusion
├── gemm/            # Hopper-aware matmul with TMA
├── indexing/        # Gather/scatter operations
├── layernorm/       # LayerNorm (Triton + CuTeDSL)
├── loss/            # Cross-entropy, softmax losses
├── optim/           # AdamW, Muon optimizers
├── rmsnorm/         # RMSNorm with fused backward
├── rotary/          # Rotary position embeddings
├── sampling/        # Top-k/top-p token sampling
├── softmax/         # Standalone softmax kernels
├── vibe_attention/  # Flash attention (SM90/SM100)
└── ops/             # High-level PyTorch module wrappers
```

Each kernel module contains:
- `kernel.py` - Implementation and Python wrapper
- `benchmark.py` - CLI benchmark harness
- `README.md` - Kernel-specific documentation

## Benchmarks

Run benchmarks for any kernel:

```bash
python -m vibe_kernels.gemm.benchmark --warmup 1 --iterations 1 1 1
python -m vibe_kernels.attention.benchmark --batch 2 --heads 4 --seqlen 128 --headdim 64 --causal --warmup 1 --iters 1
python -m vibe_kernels.rmsnorm.benchmark --rows 1024 --hidden 2048 --dtype float16 --backends torch,triton --reference torch --warmup 1 --iters 1
python -m vibe_kernels.loss.benchmark --batch 4 --seq 128 --vocab 2048 --dtype float16 --warmup 1 --iters 1
```

### Performance Summary (H100 PCIe, BF16)

| Kernel | vs PyTorch | Best Backend |
|--------|------------|--------------|
| RMSNorm | **5.8x** | Triton |
| Rotary | **7.9x** | Triton |
| Cross-Entropy | **6.0x** | CuTeDSL |
| Softmax | **2.3x** | CuTeDSL |
| AdamW | **2.0x** | Triton |
| Attention | **1.5x** | Triton |

See [BENCHMARKS.md](BENCHMARKS.md) for detailed results.

## Backend Selection

Several kernels support multiple backends:

```python
import torch
from vibe_kernels.loss import softmax, log_softmax

x = torch.randn(2, 128, device="cuda", dtype=torch.float16)

# Choose backend: "triton", "cutedsl", or "torch"
out = softmax(x, backend="triton")
```

## Dependencies

**Triton kernels**: Most Triton-based kernels depend on PyTorch for tensor operations and CUDA device detection.

**PyTorch-free usage**: Most kernel modules provide a `vbt_native.py` implementation (VibeTensor + Triton PTX) that can run without PyTorch. These are used in the [VibeTensor examples](../examples/). The `ops/` directory contains higher-level building blocks.

**CuTeDSL kernels**: Do not have VibeTensor bindings yet and require PyTorch.

## Development

Add new kernels following the existing structure:
1. Create `kernel.py` with Triton/CuTeDSL implementation
2. Add `benchmark.py` for performance testing
3. Include tests in `tests/` subdirectory
4. Export from module `__init__.py`


